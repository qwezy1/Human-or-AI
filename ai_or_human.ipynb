{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "330eff4f",
   "metadata": {},
   "source": [
    "# –∏–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0beb4765",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\–º–∞—à–∞\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\–º–∞—à–∞\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\–º–∞—à–∞\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a96b715",
   "metadata": {},
   "source": [
    "# –∑–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤ –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c10e4ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–í –ì—Ä—É–∑–∏–∏ –±—ã–ª –∑–∞–¥–µ—Ä–∂–∞–Ω –≥—Ä–∞–∂–¥–∞–Ω–∏–Ω –†–æ—Å—Å–∏–∏, –ø–æ–¥–æ–∑—Ä...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–ó–∞–∫–æ–Ω –æ–± –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —É–∫—Ä–∞–∏–Ω—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ –º–æ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>–†–æ—Å—Å–∏–π—Å–∫–æ–π –≤–∞–ª—é—Ç–µ –Ω–µ —Å—Ç–æ–∏—Ç –æ–∂–∏–¥–∞—Ç—å —É–∫—Ä–µ–ø–ª–µ–Ω–∏—è ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4 —Å–µ–Ω—Ç—è–±—Ä—è –≤ –ú–æ—Å–∫–≤–µ —Å–æ—Å—Ç–æ—è–ª–æ—Å—å —Ç–æ—Ä–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –æ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–í 2022 –≥–æ–¥—É –≤ –†–æ—Å—Å–∏–∏ –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω—ã –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  –í –ì—Ä—É–∑–∏–∏ –±—ã–ª –∑–∞–¥–µ—Ä–∂–∞–Ω –≥—Ä–∞–∂–¥–∞–Ω–∏–Ω –†–æ—Å—Å–∏–∏, –ø–æ–¥–æ–∑—Ä...      1\n",
       "1  –ó–∞–∫–æ–Ω –æ–± –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —É–∫—Ä–∞–∏–Ω—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ –º–æ...      0\n",
       "2  –†–æ—Å—Å–∏–π—Å–∫–æ–π –≤–∞–ª—é—Ç–µ –Ω–µ —Å—Ç–æ–∏—Ç –æ–∂–∏–¥–∞—Ç—å —É–∫—Ä–µ–ø–ª–µ–Ω–∏—è ...      0\n",
       "3  4 —Å–µ–Ω—Ç—è–±—Ä—è –≤ –ú–æ—Å–∫–≤–µ —Å–æ—Å—Ç–æ—è–ª–æ—Å—å —Ç–æ—Ä–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –æ...      1\n",
       "4  –í 2022 –≥–æ–¥—É –≤ –†–æ—Å—Å–∏–∏ –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω—ã –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig = pd.read_json(\"original_news.json\")\n",
    "df_gen  = pd.read_json(\"generated_news.json\")\n",
    "\n",
    "df_orig = df_orig.rename(columns={\"original_news\": \"text\"})\n",
    "df_gen  = df_gen.rename(columns={\"generated_news\": \"text\"})\n",
    "df_orig[\"label\"] = 0\n",
    "df_gen[\"label\"] = 1\n",
    "\n",
    "df = pd.concat([df_orig, df_gen], ignore_index=True)\n",
    "df = df[['text', 'label']]\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8b308cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1438a6",
   "metadata": {},
   "source": [
    "# —Ñ—É–Ω–∫—Ü–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç–∏–ª–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59efea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_stylo_features(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    avg_sent_len = np.mean([len(word_tokenize(s)) for s in sentences]) if sentences else 0\n",
    "    sent_len_var = np.var([len(word_tokenize(s)) for s in sentences]) if sentences else 0\n",
    "    ttr = len(set(words)) / len(words) if words else 0\n",
    "    markdown_bold = text.count(\"**\")\n",
    "\n",
    "    return pd.Series({\n",
    "        \"avg_sent_len\": avg_sent_len,\n",
    "        \"sent_len_var\": sent_len_var,\n",
    "        \"ttr\": ttr,\n",
    "        \"markdown_bold\": markdown_bold\n",
    "    })\n",
    "\n",
    "df_stylo = X.apply(extract_stylo_features)\n",
    "df_features = pd.concat([df[['text']], df_stylo], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e611d6d",
   "metadata": {},
   "source": [
    "# —Å–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "965dc308",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    max_features=30000,\n",
    "    ngram_range=(1,2),\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "    (\"tfidf\", tfidf, \"text\"),\n",
    "    (\"stylo\", StandardScaler(), [\"avg_sent_len\", \"sent_len_var\", \"ttr\", \"markdown_bold\"]),\n",
    "])\n",
    "\n",
    "model = Pipeline([\n",
    "    ('features', ct),\n",
    "    ('clf', LogisticRegression(max_iter=400, class_weight=\"balanced\"))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f346aae0",
   "metadata": {},
   "source": [
    "# –æ–±—É—á–µ–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ce3aef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9270833333333334\n",
      "F1-score: 0.9195402298850575\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93       106\n",
      "           1       0.91      0.93      0.92        86\n",
      "\n",
      "    accuracy                           0.93       192\n",
      "   macro avg       0.93      0.93      0.93       192\n",
      "weighted avg       0.93      0.93      0.93       192\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_features, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1b41d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ –ú–∞—Ä–∫–µ—Ä—ã AI:\n",
      "–∏–Ω—Ü–∏–¥–µ–Ω—Ç 0.6905\n",
      "–æ–¥–Ω–∞–∫–æ 0.6749\n",
      "—ç—Ç–æ—Ç 0.6451\n",
      "—ç—Ç–æ 0.6394\n",
      "–¥–ª—è 0.6372\n",
      "–Ω–µ —Ç–æ–ª—å–∫–æ 0.5968\n",
      "–º—ã 0.5765\n",
      "–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ 0.5508\n",
      "–Ω–µ—Å–º–æ—Ç—Ä—è 0.538\n",
      "–Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ 0.538\n",
      "–≤–∫–ª—é—á–∞—è 0.5303\n",
      "–æ—Å—Ç–∞–µ—Ç—Å—è 0.5263\n",
      "—ç–∫—Å–ø–µ—Ä—Ç—ã 0.5118\n",
      "—Ç–æ–ª—å–∫–æ 0.5086\n",
      "–ø–æ–¥–æ–±–Ω—ã–µ 0.5075\n",
      "–æ—Å–æ–±–µ–Ω–Ω–æ 0.4755\n",
      "–≤–Ω–∏–º–∞–Ω–∏–µ 0.4708\n",
      "–ø—Ä–æ–∏–∑–æ—à–µ–¥—à–µ–≥–æ 0.4668\n",
      "–¥–∞–Ω–Ω—ã–π 0.4623\n",
      "–ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç 0.4407\n",
      "\n",
      "üîπ –ú–∞—Ä–∫–µ—Ä—ã Human:\n",
      "—Å–æ–æ–±—â–∞–µ—Ç -0.7498\n",
      "–æ–± -0.6957\n",
      "–æ–± —ç—Ç–æ–º -0.6913\n",
      "—ç—Ç–æ–º -0.661\n",
      "–ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ -0.6268\n",
      "–ø—Ä–æ—Ü–µ–Ω—Ç–∞ -0.5356\n",
      "–≥–æ–¥–∞ -0.4561\n",
      "—Å–æ —Å—Å—ã–ª–∫–æ–π -0.4047\n",
      "—Å—Å—ã–ª–∫–æ–π –Ω–∞ -0.4041\n",
      "—Å—Å—ã–ª–∫–æ–π -0.4035\n",
      "—Ä–∏–∞ –Ω–æ–≤–æ—Å—Ç–∏ -0.4011\n",
      "—Ä–∏–∞ -0.4011\n",
      "–ø—Ä–∏ —ç—Ç–æ–º -0.3787\n",
      "–Ω–æ–≤–æ—Å—Ç–∏ -0.3766\n",
      "the -0.3752\n",
      "–ø–∏—à–µ—Ç -0.37\n",
      "—ç—Ç–æ–º —Å–æ–æ–±—â–∞–µ—Ç -0.37\n",
      "—Å–æ–æ–±—â–∞–ª–æ—Å—å -0.3636\n",
      "—Å–∫–∞–∑–∞–ª -0.3621\n",
      "–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞ -0.3596\n"
     ]
    }
   ],
   "source": [
    "feature_names = model.named_steps['features'].named_transformers_['tfidf'].get_feature_names_out()\n",
    "coefs = model.named_steps['clf'].coef_[0][:len(feature_names)]\n",
    "\n",
    "top_ai = np.argsort(coefs)[-20:][::-1]  \n",
    "top_human = np.argsort(coefs)[:20]      \n",
    "\n",
    "print(\"\\nüîπ –ú–∞—Ä–∫–µ—Ä—ã AI:\")\n",
    "for i in top_ai:\n",
    "    print(feature_names[i], round(coefs[i], 4))\n",
    "\n",
    "print(\"\\nüîπ –ú–∞—Ä–∫–µ—Ä—ã Human:\")\n",
    "for i in top_human:\n",
    "    print(feature_names[i], round(coefs[i], 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "314203df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°—Ç–∏–ª–æ–º–µ—Ç—Ä–∏—è (–≤–ª–∏—è–Ω–∏–µ –Ω–∞ –∫–ª–∞—Å—Å AI):\n",
      "avg_sent_len: 1.0644\n",
      "sent_len_var: -1.7677\n",
      "ttr: -0.8524\n",
      "markdown_bold: 2.5615\n"
     ]
    }
   ],
   "source": [
    "stylo_importance = model.named_steps['clf'].coef_[0][-4:]\n",
    "stylo_feats = [\"avg_sent_len\",\"sent_len_var\",\"ttr\",\"markdown_bold\"]\n",
    "\n",
    "print(\"–°—Ç–∏–ª–æ–º–µ—Ç—Ä–∏—è (–≤–ª–∏—è–Ω–∏–µ –Ω–∞ –∫–ª–∞—Å—Å AI):\")\n",
    "for f, w in zip(stylo_feats, stylo_importance):\n",
    "    print(f\"{f}: {round(w, 4)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60479b91",
   "metadata": {},
   "source": [
    "# —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "983a4756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prediction': 'Human-written', 'confidence': 0.9510883089432539}\n"
     ]
    }
   ],
   "source": [
    "label_map = {1: \"AI-generated\", 0: \"Human-written\"}\n",
    "\n",
    "def classify_text(text: str):\n",
    "    sample = pd.DataFrame([{\n",
    "        \"text\": text,\n",
    "        **extract_stylo_features(text)\n",
    "    }])\n",
    "    pred = model.predict(sample)[0]\n",
    "    proba = model.predict_proba(sample)[0][1]\n",
    "    label = label_map[pred]\n",
    "    confidence = proba if pred == 1 else 1 - proba\n",
    "    return {\"prediction\": label, \"confidence\": float(confidence)}\n",
    "\n",
    "# –ø—Ä–∏–º–µ—Ä\n",
    "example = '''–ú–æ—Å–∫–≤–∞, 30 –Ω–æ—è–±—Ä—è 2025 –≥–æ–¥–∞. –†–æ—Å–∞—Ç–æ–º –æ–±—ä—è–≤–∏–ª –æ –≤–≤–æ–¥–µ –≤ –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—É—é —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏—é –ø–ª–∞–≤—É—á–µ–π –∞—Ç–æ–º–Ω–æ–π —Ç–µ–ø–ª–æ—ç–ª–µ–∫—Ç—Ä–æ—Å—Ç–∞–Ω—Ü–∏–∏ (–ü–ê–¢–≠–°) –ø—Ä–æ–µ–∫—Ç–∞ 2025 ¬´–ê–∫–∞–¥–µ–º–∏–∫ –õ–æ–º–æ–Ω–æ—Å–æ–≤-2¬ª –≤ –∞–∫–≤–∞—Ç–æ—Ä–∏–∏ –ü–µ–≤–µ–∫–∞ (–ß—É–∫–æ—Ç—Å–∫–∏–π –ê–û).\n",
    "–ù–æ–≤–∞—è —Å—Ç–∞–Ω—Ü–∏—è –æ—Å–Ω–∞—â–µ–Ω–∞ –¥–≤—É–º—è —Ä–µ–∞–∫—Ç–æ—Ä–∞–º–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è –†–ò–¢–ú-200–ú –º–æ—â–Ω–æ—Å—Ç—å—é –ø–æ 60 –ú–í—Ç –∫–∞–∂–¥—ã–π –∏ —Å–ø–æ—Å–æ–±–Ω–∞ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –≤—ã—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –¥–æ 120 –ú–í—Ç —ç–ª–µ–∫—Ç—Ä–æ—ç–Ω–µ—Ä–≥–∏–∏ –∏ –¥–æ 100 –ì–∫–∞–ª/—á —Ç–µ–ø–ª–∞. –ü–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø–µ—Ä–≤–æ–π –ü–ê–¢–≠–° (¬´–ê–∫–∞–¥–µ–º–∏–∫ –õ–æ–º–æ–Ω–æ—Å–æ–≤¬ª, –∑–∞–ø—É—â–µ–Ω –≤ 2019 –≥–æ–¥—É), –Ω–æ–≤–∞—è —Å—Ç–∞–Ω—Ü–∏—è –Ω–∞ 30 % –∫–æ–º–ø–∞–∫—Ç–Ω–µ–µ, –Ω–∞ 20 % —ç–∫–æ–Ω–æ–º–∏—á–Ω–µ–µ –ø–æ —Ç–æ–ø–ª–∏–≤—É –∏ –∏–º–µ–µ—Ç —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π —Å—Ä–æ–∫ —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏ –¥–æ 60 –ª–µ—Ç –±–µ–∑ –ø–µ—Ä–µ–∑–∞—Ä—è–¥–∫–∏.'''\n",
    "print(classify_text(example))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36f1ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(model, \"model.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
