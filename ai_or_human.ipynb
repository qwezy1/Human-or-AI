{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "330eff4f",
   "metadata": {},
   "source": [
    "# –∏–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0beb4765",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\–º–∞—à–∞\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\–º–∞—à–∞\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\–º–∞—à–∞\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a96b715",
   "metadata": {},
   "source": [
    "# –∑–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤ –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10e4ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_jsons(root_dir: str, include_long_sc: bool = False):\n",
    "    root = Path(root_dir)\n",
    "    patterns = [\"main/**/*.json\"]\n",
    "    records = []\n",
    "    for pat in patterns:\n",
    "        for fp in root.glob(pat):\n",
    "            try:\n",
    "                with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "            except Exception as e:\n",
    "                print(f\"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {fp}: {e}\")\n",
    "                continue\n",
    "            for it in data:\n",
    "                rec = dict(it)\n",
    "                rec[\"_source_file\"] = str(fp.relative_to(root))\n",
    "                records.append(rec)\n",
    "    return records\n",
    "\n",
    "RE_WS = re.compile(r\"\\s+\", flags=re.UNICODE)\n",
    "def clean_text(s):\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = s.strip()\n",
    "    s = RE_WS.sub(\" \", s)\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "be6e80f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: 4317\n"
     ]
    }
   ],
   "source": [
    "records = load_all_jsons('data')\n",
    "print(\"–ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π:\", len(records))\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "for col in [\"id\",\"text\",\"dataset\",\"source\",\"model\",\"paraphrasing_type\"]:\n",
    "    if col not in df.columns:\n",
    "        df[col] = None\n",
    "\n",
    "# –º–µ—Ç–∫–∏: human=0, ai & ai+rew = 1\n",
    "df[\"label\"] = df[\"source\"].apply(lambda s: 0 if s == \"human\" else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d8b308cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1438a6",
   "metadata": {},
   "source": [
    "# —Ñ—É–Ω–∫—Ü–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç–∏–ª–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "59efea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_stylo_features(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    avg_sent_len = np.mean([len(word_tokenize(s)) for s in sentences]) if sentences else 0\n",
    "    sent_len_var = np.var([len(word_tokenize(s)) for s in sentences]) if sentences else 0\n",
    "    ttr = len(set(words)) / len(words) if words else 0\n",
    "    markdown_bold = text.count(\"**\")\n",
    "\n",
    "    return pd.Series({\n",
    "        \"avg_sent_len\": avg_sent_len,\n",
    "        \"sent_len_var\": sent_len_var,\n",
    "        \"ttr\": ttr,\n",
    "        \"markdown_bold\": markdown_bold\n",
    "    })\n",
    "\n",
    "df_stylo = X.apply(extract_stylo_features)\n",
    "df_features = pd.concat([df[['text']], df_stylo], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e611d6d",
   "metadata": {},
   "source": [
    "# —Å–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "965dc308",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    max_features=30000,\n",
    "    ngram_range=(1,2),\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "    (\"tfidf\", tfidf, \"text\"),\n",
    "    (\"stylo\", StandardScaler(), [\"avg_sent_len\", \"sent_len_var\", \"ttr\", \"markdown_bold\"]),\n",
    "])\n",
    "\n",
    "model = Pipeline([\n",
    "    ('features', ct),\n",
    "    ('clf', LogisticRegression(max_iter=400, class_weight=\"balanced\"))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f346aae0",
   "metadata": {},
   "source": [
    "# –æ–±—É—á–µ–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8ce3aef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8206018518518519\n",
      "F1-score: 0.8541862652869238\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.93      0.77       273\n",
      "           1       0.96      0.77      0.85       591\n",
      "\n",
      "    accuracy                           0.82       864\n",
      "   macro avg       0.81      0.85      0.81       864\n",
      "weighted avg       0.86      0.82      0.83       864\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_features, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a1b41d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ –ú–∞—Ä–∫–µ—Ä—ã AI:\n",
      "–æ–¥–Ω–∞–∫–æ 1.7649\n",
      "–≤–∫–ª—é—á–∞—è 1.6421\n",
      "–µ—ë 1.5101\n",
      "–∏–Ω—Ñ–æ—Ä–º–∏—Ä—É–µ—Ç 1.4952\n",
      "—ç—Ç–æ—Ç 1.4837\n",
      "–¥–ª—è 1.4335\n",
      "–Ω–µ —Ç–æ–ª—å–∫–æ 1.2585\n",
      "—ç—Ç–æ–º –∏–Ω—Ñ–æ—Ä–º–∏—Ä—É–µ—Ç 1.2147\n",
      "–∫–∞–∫ 1.1527\n",
      "–æ—Å–æ–±–µ–Ω–Ω–æ 1.1425\n",
      "–Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ 1.1242\n",
      "–Ω–µ—Å–º–æ—Ç—Ä—è 1.1023\n",
      "–≤–∞–∂–Ω–æ 1.0976\n",
      "–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç 1.0758\n",
      "–ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç 1.0614\n",
      "–≤–æ–ø—Ä–æ—Å 1.0315\n",
      "—ç—Ç–æ 1.0279\n",
      "—ç—Ç–∏ 1.007\n",
      "—Å—Å—ã–ª–∞—è—Å—å 0.9596\n",
      "—Å—Å—ã–ª–∞—è—Å—å –Ω–∞ 0.9596\n",
      "\n",
      "üîπ –ú–∞—Ä–∫–µ—Ä—ã Human:\n",
      "—Å–æ–æ–±—â–∞–µ—Ç -2.1748\n",
      "—ç—Ç–æ–º —Å–æ–æ–±—â–∞–µ—Ç -1.6725\n",
      "–æ–± -1.6064\n",
      "–æ—á–µ–Ω—å -1.4899\n",
      "—Å–æ —Å—Å—ã–ª–∫–æ–π -1.4605\n",
      "—Å—Å—ã–ª–∫–æ–π -1.4431\n",
      "—Å—Å—ã–ª–∫–æ–π –Ω–∞ -1.4153\n",
      "–æ–± —ç—Ç–æ–º -1.4138\n",
      "—á–∏—Å–ª–µ -1.3866\n",
      "–ø–∏—à–µ—Ç -1.3734\n",
      "—Ç–æ–º —á–∏—Å–ª–µ -1.3657\n",
      "–ø–æ—Ç–æ–º—É -1.3353\n",
      "–ø—Ä–æ–±–ª–µ–º—É -1.2053\n",
      "—Å–∫–∞–∑–∞–ª -1.1995\n",
      "–ø–æ—Ç–æ–º—É —á—Ç–æ -1.1925\n",
      "—Å–æ -1.1635\n",
      "–ø–æ -1.1587\n",
      "–≥–≥ -1.1545\n",
      "—Å–æ–æ–±—â–∞–µ—Ç—Å—è -1.1541\n",
      "–Ω–∞ —Å–∞–π—Ç–µ -1.1127\n"
     ]
    }
   ],
   "source": [
    "feature_names = model.named_steps['features'].named_transformers_['tfidf'].get_feature_names_out()\n",
    "coefs = model.named_steps['clf'].coef_[0][:len(feature_names)]\n",
    "\n",
    "top_ai = np.argsort(coefs)[-20:][::-1]  \n",
    "top_human = np.argsort(coefs)[:20]      \n",
    "\n",
    "print(\"\\nüîπ –ú–∞—Ä–∫–µ—Ä—ã AI:\")\n",
    "for i in top_ai:\n",
    "    print(feature_names[i], round(coefs[i], 4))\n",
    "\n",
    "print(\"\\nüîπ –ú–∞—Ä–∫–µ—Ä—ã Human:\")\n",
    "for i in top_human:\n",
    "    print(feature_names[i], round(coefs[i], 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "314203df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°—Ç–∏–ª–æ–º–µ—Ç—Ä–∏—è (–≤–ª–∏—è–Ω–∏–µ –Ω–∞ –∫–ª–∞—Å—Å AI):\n",
      "avg_sent_len: 0.4925\n",
      "sent_len_var: -0.8599\n",
      "ttr: 0.2969\n",
      "markdown_bold: 6.9352\n"
     ]
    }
   ],
   "source": [
    "stylo_importance = model.named_steps['clf'].coef_[0][-4:]\n",
    "stylo_feats = [\"avg_sent_len\",\"sent_len_var\",\"ttr\",\"markdown_bold\"]\n",
    "\n",
    "print(\"–°—Ç–∏–ª–æ–º–µ—Ç—Ä–∏—è (–≤–ª–∏—è–Ω–∏–µ –Ω–∞ –∫–ª–∞—Å—Å AI):\")\n",
    "for f, w in zip(stylo_feats, stylo_importance):\n",
    "    print(f\"{f}: {round(w, 4)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60479b91",
   "metadata": {},
   "source": [
    "# —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "983a4756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prediction': 'AI-generated', 'confidence': 0.8843162495168472}\n"
     ]
    }
   ],
   "source": [
    "label_map = {1: \"AI-generated\", 0: \"Human-written\"}\n",
    "\n",
    "def classify_text(text: str):\n",
    "    sample = pd.DataFrame([{\n",
    "        \"text\": text,\n",
    "        **extract_stylo_features(text)\n",
    "    }])\n",
    "    pred = model.predict(sample)[0]\n",
    "    proba = model.predict_proba(sample)[0][1]\n",
    "    label = label_map[pred]\n",
    "    confidence = proba if pred == 1 else 1 - proba\n",
    "    return {\"prediction\": label, \"confidence\": float(confidence)}\n",
    "\n",
    "# –ø—Ä–∏–º–µ—Ä\n",
    "example = '''\n",
    "\n",
    "**–ú–æ—Å–∫–≤–∞, 15 –º–∞—Ä—Ç–∞ 2024 –≥.** ‚Äî –ì—Ä—É–ø–ø–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π –∏–∑ –ù–∞—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ —Ü–µ–Ω—Ç—Ä–∞ ¬´–ö—É—Ä—á–∞—Ç–æ–≤—Å–∫–∏–π –∏–Ω—Å—Ç–∏—Ç—É—Ç¬ª –∏ –ú–ò–§–ò –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∞ –ø—Ä–æ—Ç–æ—Ç–∏–ø –∫–≤–∞–Ω—Ç–æ–≤–æ–≥–æ –∫–æ–º–ø—å—é—Ç–µ—Ä–∞, –∫–ª—é—á–µ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∫–æ—Ç–æ—Ä–æ–≥–æ ‚Äî –∫—É–±–∏—Ç—ã –∏ —É–ø—Ä–∞–≤–ª—è—é—â–∞—è —ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞ ‚Äî –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω—ã –∏ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω—ã –≤ –†–æ—Å—Å–∏–∏.\n",
    "\n",
    "–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ, –ø–æ–ª—É—á–∏–≤—à–µ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ ¬´–ö–≤–∞–Ω—Ç-1¬ª, –ø–æ–∫–∞ –æ–±–ª–∞–¥–∞–µ—Ç —Å–∫—Ä–æ–º–Ω–æ–π –º–æ—â–Ω–æ—Å—Ç—å—é –≤ 8 –∫—É–±–∏—Ç–æ–≤, –Ω–æ –µ–≥–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å —Å–∏—Å—Ç–µ–º—É –¥–æ —Å–æ—Ç–µ–Ω –∫—É–±–∏—Ç–æ–≤ –≤ –±–ª–∏–∂–∞–π—à–∏–µ —Ç—Ä–∏ –≥–æ–¥–∞. –ì–ª–∞–≤–Ω—ã–º –ø—Ä–æ—Ä—ã–≤–æ–º —É—á–µ–Ω—ã–µ –Ω–∞–∑—ã–≤–∞—é—Ç —Å–æ–∑–¥–∞–Ω–∏–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–π —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞ —Å–≤–µ—Ä—Ö–ø—Ä–æ–≤–æ–¥—è—â–∏—Ö –∫—É–±–∏—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–ª—é–º–∏–Ω–∏–µ–≤—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä, –∫–æ—Ç–æ—Ä—ã–µ —Å–ø–æ—Å–æ–±–Ω—ã —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –∫–≤–∞–Ω—Ç–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Ä–µ–∫–æ—Ä–¥–Ω–æ–µ –¥–ª—è –æ—Ç–µ—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–∞–∑—Ä–∞–±–æ—Ç–æ–∫ –≤—Ä–µ–º—è ‚Äî –±–æ–ª–µ–µ 100 –º–∏–∫—Ä–æ—Å–µ–∫—É–Ω–¥.\n",
    "\n",
    "¬´–≠—Ç–æ –Ω–µ –ø—Ä–æ—Å—Ç–æ –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω—ã–π –º–∞–∫–µ—Ç. –ú—ã –ø–æ—Å—Ç—Ä–æ–∏–ª–∏ –ø–æ–ª–Ω—ã–π —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Ü–∏–∫–ª: –æ—Ç –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞ —á–∏–ø–æ–≤ –¥–æ —Å–∏—Å—Ç–µ–º—ã —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ —Å—Ç–µ–∫–∞. –≠—Ç–æ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç –¥–ª—è –±—É–¥—É—â–µ–≥–æ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–≥–æ –∫–≤–∞–Ω—Ç–æ–≤–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞¬ª, ‚Äî –∑–∞—è–≤–∏–ª —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å –ø—Ä–æ–µ–∫—Ç–∞, –∞–∫–∞–¥–µ–º–∏–∫ –ò–≤–∞–Ω –ü–µ—Ç—Ä–æ–≤.\n",
    "\n",
    "–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –≤–µ–ª–∞—Å—å –≤ —Ä–∞–º–∫–∞—Ö –Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞ ¬´–ù–∞—É–∫–∞ –∏ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç—ã¬ª. –ü–µ—Ä–≤—ã–º–∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–º–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è–º–∏ ¬´–ö–≤–∞–Ω—Ç–∞-1¬ª —Å—Ç–∞–Ω—É—Ç –∑–∞–¥–∞—á–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ª–æ–≥–∏—Å—Ç–∏–∫–∏ –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤. –î–æ –∫–æ–Ω—Ü–∞ –≥–æ–¥–∞ –∫ —Å–∏—Å—Ç–µ–º–µ –ø–æ–ª—É—á–∞—Ç –¥–æ—Å—Ç—É–ø –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–æ–≤ –∏ –∫–æ–º–ø–∞–Ω–∏–π-–ø–∞—Ä—Ç–Ω–µ—Ä–æ–≤.\n",
    "\n",
    "–≠–∫—Å–ø–µ—Ä—Ç—ã –æ—Ç–º–µ—á–∞—é—Ç, —á—Ç–æ, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –æ—Ç—Å—Ç–∞–≤–∞–Ω–∏–µ –æ—Ç –ª–∏–¥–µ—Ä–æ–≤ –≤ –æ—Ç—Ä–∞—Å–ª–∏ (–∫–æ–º–ø–∞–Ω–∏–π Google –∏ IBM), —Å–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ç–µ—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã —è–≤–ª—è–µ—Ç—Å—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã–º —à–∞–≥–æ–º –¥–ª—è —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ —Å—É–≤–µ—Ä–µ–Ω–∏—Ç–µ—Ç–∞ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —Å—Ç—Ä–∞–Ω—ã –≤ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–µ.'''\n",
    "print(classify_text(example))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b36f1ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.pkl']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(model, \"model.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
